{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, concatenate, Input, Reshape\n",
    "from keras.layers import LSTM, Conv1D, Conv2D, MaxPooling1D, GRU, Dropout, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import csv\n",
    "import os\n",
    "import errno\n",
    "import operator\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('tokenizer.pickle', 'wb') as handle:\\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stem_words = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "remove_stop_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = set()\n",
    "\n",
    "with open('stopwords.txt', 'rt') as stopfile:\n",
    "    counter = 0\n",
    "    for row in stopfile:\n",
    "        stop_words.add(row.rstrip()) # if already in there, won't add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000 # 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \", char_level=False)\n",
    "# tokenizer = Tokenizer(num_words=vocab_size, filters='', lower=True, split=\" \", char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text = []\n",
    "with open('data/training_nolabel.txt', 'rt') as trainfile:\n",
    "    for idx, row in enumerate(trainfile):\n",
    "        words = []\n",
    "        for word in row.split():\n",
    "            if stem_words:\n",
    "                word = stem(word)\n",
    "                \n",
    "            if remove_stop_words and word not in stop_words:\n",
    "                words.append(word)\n",
    "            else:\n",
    "                words.append(word)\n",
    "        # print(' '.join(words))\n",
    "        train_text.append(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_text) # around 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "with open('data/training_label.txt', 'rt') as trainfile:\n",
    "    reader = csv.reader(trainfile, delimiter=' ')\n",
    "    for idx, row in enumerate(reader):\n",
    "        words = []\n",
    "        for word in row[2:]:\n",
    "            if stem_words:\n",
    "                word = stem(word)\n",
    "            \n",
    "            if remove_stop_words and word not in stop_words:\n",
    "                words.append(word)\n",
    "            else:\n",
    "                words.append(word)\n",
    "        words = ' '.join(words)\n",
    "        X_train.append(words)\n",
    "        y_train.append(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### if use word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 140, 11, 3, 2502, 8097, 11, 30]"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n"
     ]
    }
   ],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 40 # max lengths -- training_label:39    testing_data:39\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "print(X_train[0].shape) # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_110 (Embedding)       (None, 40, 20)       200000      main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 38, 100)      6100        embedding_110[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 37, 100)      8100        embedding_110[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 36, 100)      10100       embedding_110[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 35, 100)      12100       embedding_110[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 34, 100)      14100       embedding_110[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling1D) (None, 1, 100)       0           conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling1D) (None, 1, 100)       0           conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling1D) (None, 1, 100)       0           conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling1D) (None, 1, 100)       0           conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling1D) (None, 1, 100)       0           conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_84 (Flatten)            (None, 100)          0           max_pooling1d_87[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_85 (Flatten)            (None, 100)          0           max_pooling1d_88[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_86 (Flatten)            (None, 100)          0           max_pooling1d_89[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_87 (Flatten)            (None, 100)          0           max_pooling1d_90[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_88 (Flatten)            (None, 100)          0           max_pooling1d_91[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 500)          0           flatten_84[0][0]                 \n",
      "                                                                 flatten_85[0][0]                 \n",
      "                                                                 flatten_86[0][0]                 \n",
      "                                                                 flatten_87[0][0]                 \n",
      "                                                                 flatten_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 500)          0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_154 (Dense)               (None, 50)           25050       dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 50)           0           dense_154[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_155 (Dense)               (None, 1)            51          dropout_56[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 275,601\n",
      "Trainable params: 275,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "180000/180000 [==============================] - 46s 256us/step - loss: 0.4860 - acc: 0.7665 - val_loss: 0.4428 - val_acc: 0.7951\n",
      "Epoch 2/10\n",
      "180000/180000 [==============================] - 35s 196us/step - loss: 0.4210 - acc: 0.8103 - val_loss: 0.4413 - val_acc: 0.7984\n",
      "Epoch 3/10\n",
      "180000/180000 [==============================] - 35s 195us/step - loss: 0.3855 - acc: 0.8291 - val_loss: 0.4447 - val_acc: 0.7994\n",
      "Epoch 4/10\n",
      "180000/180000 [==============================] - 35s 197us/step - loss: 0.3528 - acc: 0.8443 - val_loss: 0.4588 - val_acc: 0.7940\n",
      "Epoch 5/10\n",
      "180000/180000 [==============================] - 36s 197us/step - loss: 0.3223 - acc: 0.8592 - val_loss: 0.5056 - val_acc: 0.7902\n",
      "Epoch 6/10\n",
      "180000/180000 [==============================] - 35s 197us/step - loss: 0.2967 - acc: 0.8708 - val_loss: 0.5336 - val_acc: 0.7873\n",
      "Epoch 7/10\n",
      "180000/180000 [==============================] - 36s 197us/step - loss: 0.2742 - acc: 0.8814 - val_loss: 0.5821 - val_acc: 0.7833\n",
      "Epoch 8/10\n",
      "180000/180000 [==============================] - 35s 193us/step - loss: 0.2555 - acc: 0.8896 - val_loss: 0.6183 - val_acc: 0.7797\n",
      "Epoch 9/10\n",
      "180000/180000 [==============================] - 35s 195us/step - loss: 0.2401 - acc: 0.8962 - val_loss: 0.6669 - val_acc: 0.7745\n",
      "Epoch 10/10\n",
      " 98752/180000 [===============>..............] - ETA: 15s - loss: 0.2150 - acc: 0.9080"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-552-93d0c554346a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dirs = {'model_dir': 'single_test', 'tensorboard_dir': 'single_test'}\n",
    "file_name = 'larger_vocab_cnn'\n",
    "\n",
    "for dir_key in dirs:\n",
    "    if not os.path.exists(os.getcwd() + '/' + dirs[dir_key]):\n",
    "        try:\n",
    "            os.makedirs(os.getcwd() + '/' + dirs[dir_key])\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "embedding_vector_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "main_input = Input(shape=(None,), dtype='int32', name='main_input')\n",
    "x = Embedding(input_dim=vocab_size, output_dim=embedding_vector_length, input_length=40)(main_input)\n",
    "\n",
    "convs_flattened = []\n",
    "for i in range(3,8):\n",
    "    tower = Conv1D(filters=100, kernel_size=i, activation='relu')(x)\n",
    "    # print(tower._keras_shape)\n",
    "    tower = MaxPooling1D(tower._keras_shape[1])(tower)\n",
    "    tower = Flatten()(tower)\n",
    "    convs_flattened.append(tower)\n",
    "\n",
    "conv_results = concatenate(convs_flattened)\n",
    "output = Dropout(0.5)(conv_results)\n",
    "output = Dense(50, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(inputs=main_input, outputs=output)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "save_model = ModelCheckpoint(dirs['model_dir'] + '/' + file_name + '-{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "# tensorboard = TensorBoard(log_dir=dirs['tensorboard_dir'], histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=10, batch_size=batch_size, callbacks=[save_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_115 (Embedding)    (None, 40, 20)            200000    \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 40, 32)            5152      \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 40, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 40, 32)            8224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 20, 32)            8224      \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 20, 32)            8224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_61 (CuDNNGRU)      (None, 10, 100)           40200     \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 10, 100)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_62 (CuDNNGRU)      (None, 10, 100)           60600     \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 10, 100)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_63 (CuDNNGRU)      (None, 10, 100)           60600     \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 10, 100)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_64 (CuDNNGRU)      (None, 100)               60600     \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 472,125\n",
      "Trainable params: 472,125\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "180000/180000 [==============================] - 63s 352us/step - loss: 0.5213 - acc: 0.7267 - val_loss: 0.4533 - val_acc: 0.7958\n",
      "Epoch 2/10\n",
      "180000/180000 [==============================] - 49s 270us/step - loss: 0.4215 - acc: 0.8119 - val_loss: 0.4295 - val_acc: 0.8044\n",
      "Epoch 3/10\n",
      "180000/180000 [==============================] - 49s 270us/step - loss: 0.3884 - acc: 0.8309 - val_loss: 0.4381 - val_acc: 0.8020\n",
      "Epoch 4/10\n",
      "180000/180000 [==============================] - 49s 271us/step - loss: 0.3579 - acc: 0.8464 - val_loss: 0.4587 - val_acc: 0.7876\n",
      "Epoch 5/10\n",
      " 39296/180000 [=====>........................] - ETA: 36s - loss: 0.3175 - acc: 0.8669"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-561-5012838e0de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "dirs = {'model_dir': 'single_test', 'tensorboard_dir': 'single_test'}\n",
    "file_name = 'big_conv_pool_GRU_highdrop'\n",
    "\n",
    "for dir_key in dirs:\n",
    "    if not os.path.exists(os.getcwd() + '/' + dirs[dir_key]):\n",
    "        try:\n",
    "            os.makedirs(os.getcwd() + '/' + dirs[dir_key])\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "batch_size = 64\n",
    "embedding_vector_length = 20\n",
    "model = Sequential()\n",
    "\n",
    "# input an integer matrix of size (batch, input_length)\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_vector_length, input_length=max_review_length))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=8, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(CuDNNGRU(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNGRU(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNGRU(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNGRU(100))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "save_model = ModelCheckpoint(dirs['model_dir'] + '/' + file_name + '-{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "# tensorboard = TensorBoard(log_dir=dirs['tensorboard_dir'], histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=10, batch_size=batch_size, callbacks=[save_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### if use BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "print(X_train.shape) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 43)                215043    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 43)                1892      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 44        \n",
      "=================================================================\n",
      "Total params: 216,979\n",
      "Trainable params: 216,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "180000/180000 [==============================] - 18s 98us/step - loss: 0.4958 - acc: 0.7716 - val_loss: 0.4837 - val_acc: 0.7802\n",
      "Epoch 2/10\n",
      "180000/180000 [==============================] - 17s 95us/step - loss: 0.4732 - acc: 0.7866 - val_loss: 0.4823 - val_acc: 0.7805\n",
      "Epoch 3/10\n",
      "180000/180000 [==============================] - 16s 91us/step - loss: 0.4689 - acc: 0.7889 - val_loss: 0.4797 - val_acc: 0.7828\n",
      "Epoch 4/10\n",
      "180000/180000 [==============================] - 16s 91us/step - loss: 0.4664 - acc: 0.7904 - val_loss: 0.4803 - val_acc: 0.7833\n",
      "Epoch 5/10\n",
      "180000/180000 [==============================] - 17s 97us/step - loss: 0.4647 - acc: 0.7912 - val_loss: 0.4803 - val_acc: 0.7835\n",
      "Epoch 6/10\n",
      "180000/180000 [==============================] - 17s 95us/step - loss: 0.4633 - acc: 0.7916 - val_loss: 0.4802 - val_acc: 0.7836\n",
      "Epoch 7/10\n",
      "180000/180000 [==============================] - 17s 95us/step - loss: 0.4622 - acc: 0.7926 - val_loss: 0.4799 - val_acc: 0.7812\n",
      "Epoch 8/10\n",
      "180000/180000 [==============================] - 18s 98us/step - loss: 0.4612 - acc: 0.7933 - val_loss: 0.4802 - val_acc: 0.7805\n",
      "Epoch 9/10\n",
      "180000/180000 [==============================] - 17s 97us/step - loss: 0.4606 - acc: 0.7941 - val_loss: 0.4802 - val_acc: 0.7819\n",
      "Epoch 10/10\n",
      "180000/180000 [==============================] - 18s 98us/step - loss: 0.4598 - acc: 0.7951 - val_loss: 0.4813 - val_acc: 0.7832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f590907ca90>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "batch_size = 64\n",
    "model = Sequential()\n",
    "epochs = 10\n",
    "\n",
    "# CNN before LSTM layer\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# total param should be around 213,301 to compare with LSTM\n",
    "model.add(Dense(43, input_shape=X_train[0].shape))\n",
    "model.add(Dense(43, input_shape=X_train[0].shape))\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=X_train[0].shape))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "save_model = ModelCheckpoint('punct_BOW_models/{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "tensorboard = TensorBoard(log_dir='./punct_bow_logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, callbacks=[save_model, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06-0.78.hdf5 loaded!\n"
     ]
    }
   ],
   "source": [
    "ensemble = False\n",
    "\n",
    "model_name = '06-0.78.hdf5'\n",
    "loaded_model = load_model(model_name)\n",
    "print(model_name, 'loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.66471052]\n",
      " [ 0.66471052]]\n"
     ]
    }
   ],
   "source": [
    "# custom input\n",
    "max_review_length = 40\n",
    "x_submission = tokenizer.texts_to_matrix(['today is a good day, but it is hot', 'today is hot, but it is a good day'])   \n",
    "pred = loaded_model.predict(x_submission)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading single_test/cnn-03-0.80.hdf5\n",
      "finished loading single_test/cnn_GRU-03-0.80.hdf5\n",
      "finished loading single_test/cnn_pool_GRU-02-0.80.hdf5\n",
      "finished loading single_test/cudnnGRU-05-0.80.hdf5\n",
      "finished loading single_test/four_layer_GRU-04-0.80.hdf5\n",
      "finished loading single_test/two_layer_GRU-05-0.80.hdf5\n",
      "FINISHED LOADING ALL MODELS!\n"
     ]
    }
   ],
   "source": [
    "ensemble = True\n",
    "\n",
    "models = []\n",
    "model_names = ['single_test/cnn-03-0.80.hdf5', 'single_test/cnn_GRU-03-0.80.hdf5',\n",
    "               'single_test/cnn_pool_GRU-02-0.80.hdf5', 'single_test/cudnnGRU-05-0.80.hdf5',\n",
    "              'single_test/four_layer_GRU-04-0.80.hdf5', 'single_test/two_layer_GRU-05-0.80.hdf5']\n",
    "for model_name in model_names:\n",
    "    loaded_model = load_model(model_name)\n",
    "    models.append(loaded_model)\n",
    "    print('finished loading', model_name)\n",
    "print('FINISHED LOADING ALL MODELS!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## append semi-supervised data to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_text = tokenizer.texts_to_sequences(train_text) \n",
    "train_text = sequence.pad_sequences(train_text, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1178614, 40)\n"
     ]
    }
   ],
   "source": [
    "print(train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178614/1178614 [==============================] - 17s 15us/step\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.predict(train_text, batch_size=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "margin = 0.2 # 0.2:0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished appending!\n"
     ]
    }
   ],
   "source": [
    "semi_supervised_X = []\n",
    "semi_supervised_y = []\n",
    "for idx, pred in enumerate(preds):\n",
    "    if pred[0] > 1-margin:\n",
    "        semi_supervised_X.append(train_text[idx])\n",
    "        semi_supervised_y.append(1)\n",
    "    elif pred[0] < margin:\n",
    "        semi_supervised_X.append(train_text[idx])\n",
    "        semi_supervised_y.append(0)\n",
    "print('finished appending!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712673, 40)\n",
      "(200000, 40)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(semi_supervised_X).shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = np.append(X_train, semi_supervised_X, axis=0) # 200,000 --> 912,673\n",
    "y_train = np.append(y_train, semi_supervised_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912673,)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## read test file, generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading file\n"
     ]
    }
   ],
   "source": [
    "x_submission = []\n",
    "\n",
    "with open('data/testing_data.txt', 'rt') as testfile:\n",
    "    reader = csv.reader(testfile, delimiter=',')\n",
    "    next(reader) # skip headings\n",
    "    for row in reader:\n",
    "        # print(''.join(row[1:]))\n",
    "        x_submission.append(''.join(row[1:]))\n",
    "print('finished reading file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 40)\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 40\n",
    "x_submission = tokenizer.texts_to_sequences(x_submission)   \n",
    "x_submission = sequence.pad_sequences(x_submission, maxlen=max_review_length)\n",
    "print(x_submission.shape) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,   51,   10,   51,   60,\n",
       "        732,    9,   31,    2,   15, 1722,   35,  237,  630,   38, 1722,\n",
       "          1,   65,    3,  675,    6,   75, 3513], dtype=int32)"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_submission[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 6s 31us/step\n",
      "generated predictions with  <keras.models.Sequential object at 0x7f568f4f0dd8>\n"
     ]
    }
   ],
   "source": [
    "if ensemble:\n",
    "    preds = np.zeros((x_submission.shape[0], 1))\n",
    "    for loaded_model in models:\n",
    "        preds += loaded_model.predict(x_submission, batch_size=1024, verbose=1) / len(models)\n",
    "        # print('added prediction from', loaded_model)\n",
    "    print('FINISHED PREDICTION!')\n",
    "else:\n",
    "    preds = loaded_model.predict(x_submission, batch_size=1024, verbose=1)\n",
    "    print('generated predictions with ', loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07925569]\n",
      " [ 0.13937001]\n",
      " [ 0.05694776]\n",
      " ..., \n",
      " [ 0.65190792]\n",
      " [ 0.09418858]\n",
      " [ 0.16846545]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## write submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing submission!\n"
     ]
    }
   ],
   "source": [
    "with open('biggie_net.csv', 'wt') as outfile:\n",
    "    test_writer = csv.writer(outfile)\n",
    "    test_writer.writerow(['id','label'])\n",
    "    \n",
    "    counter = 0\n",
    "    for i in (preds > 0.5):\n",
    "        test_writer.writerow([counter, int(i)])\n",
    "        counter += 1\n",
    "    \n",
    "print('finished writing submission!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
