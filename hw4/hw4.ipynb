{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import csv\n",
    "import operator\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('tokenizer.pickle', 'wb') as handle:\\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\""
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \", char_level=False)\n",
    "# tokenizer = Tokenizer(num_words=vocab_size, filters='', lower=True, split=\" \", char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text = []\n",
    "with open('data/training_nolabel.txt', 'rt') as trainfile:\n",
    "    for idx, row in enumerate(trainfile):\n",
    "        train_text.append(row.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_text) # around 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "with open('data/training_label.txt', 'rt') as trainfile:\n",
    "    reader = csv.reader(trainfile, delimiter=' ')\n",
    "    for idx, row in enumerate(reader):                \n",
    "        words = ' '.join(row[2:]) \n",
    "        X_train.append(words)\n",
    "        y_train.append(row[0])\n",
    "        # print(row[0])\n",
    "        # print(words)\n",
    "        # print(len(words.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if use word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 40)\n"
     ]
    }
   ],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 40 # max lengths -- training_label:39    testing_data:39\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "print(X_train.shape) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 32)            160000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 821405 samples, validate on 91268 samples\n",
      "Epoch 1/3\n",
      "821405/821405 [==============================] - 1150s 1ms/step - loss: 0.1606 - acc: 0.9452 - val_loss: 0.0276 - val_acc: 0.9997\n",
      "Epoch 2/3\n",
      "821405/821405 [==============================] - 1147s 1ms/step - loss: 0.1347 - acc: 0.9538 - val_loss: 0.0286 - val_acc: 0.9998\n",
      "Epoch 3/3\n",
      "821405/821405 [==============================] - 1156s 1ms/step - loss: 0.1292 - acc: 0.9553 - val_loss: 0.0217 - val_acc: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d79d4dcc0>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "batch_size = 64\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "# input an integer matrix of size (batch, input_length)\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_vector_length, input_length=max_review_length))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# CNN before LSTM layer\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# dropout for configuring the input dropout and recurrent_dropout for configuring the recurrent dropout\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "save_model = ModelCheckpoint('punct_models/{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "print(model.summary()) # loss 0.1768\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=3, batch_size=batch_size, callbacks=[save_model, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if use BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "print(X_train.shape) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 43)                215043    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 43)                1892      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 44        \n",
      "=================================================================\n",
      "Total params: 216,979\n",
      "Trainable params: 216,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "180000/180000 [==============================] - 18s 98us/step - loss: 0.4958 - acc: 0.7716 - val_loss: 0.4837 - val_acc: 0.7802\n",
      "Epoch 2/10\n",
      "180000/180000 [==============================] - 17s 95us/step - loss: 0.4732 - acc: 0.7866 - val_loss: 0.4823 - val_acc: 0.7805\n",
      "Epoch 3/10\n",
      "180000/180000 [==============================] - 16s 91us/step - loss: 0.4689 - acc: 0.7889 - val_loss: 0.4797 - val_acc: 0.7828\n",
      "Epoch 4/10\n",
      "180000/180000 [==============================] - 16s 91us/step - loss: 0.4664 - acc: 0.7904 - val_loss: 0.4803 - val_acc: 0.7833\n",
      "Epoch 5/10\n",
      "180000/180000 [==============================] - 17s 97us/step - loss: 0.4647 - acc: 0.7912 - val_loss: 0.4803 - val_acc: 0.7835\n",
      "Epoch 6/10\n",
      "180000/180000 [==============================] - 17s 95us/step - loss: 0.4633 - acc: 0.7916 - val_loss: 0.4802 - val_acc: 0.7836\n",
      "Epoch 7/10\n",
      "180000/180000 [==============================] - 17s 95us/step - loss: 0.4622 - acc: 0.7926 - val_loss: 0.4799 - val_acc: 0.7812\n",
      "Epoch 8/10\n",
      "180000/180000 [==============================] - 18s 98us/step - loss: 0.4612 - acc: 0.7933 - val_loss: 0.4802 - val_acc: 0.7805\n",
      "Epoch 9/10\n",
      "180000/180000 [==============================] - 17s 97us/step - loss: 0.4606 - acc: 0.7941 - val_loss: 0.4802 - val_acc: 0.7819\n",
      "Epoch 10/10\n",
      "180000/180000 [==============================] - 18s 98us/step - loss: 0.4598 - acc: 0.7951 - val_loss: 0.4813 - val_acc: 0.7832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f590907ca90>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "batch_size = 64\n",
    "model = Sequential()\n",
    "epochs = 10\n",
    "\n",
    "# CNN before LSTM layer\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# total param should be around 213,301 to compare with LSTM\n",
    "model.add(Dense(43, input_shape=X_train[0].shape))\n",
    "model.add(Dense(43, input_shape=X_train[0].shape))\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=X_train[0].shape))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "save_model = ModelCheckpoint('punct_BOW_models/{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "tensorboard = TensorBoard(log_dir='./punct_bow_logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, callbacks=[save_model, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/03-0.80.hdf5 loaded!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'models/03-0.80.hdf5'\n",
    "loaded_model = load_model(model_name)\n",
    "print(model_name, 'loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## append semi-supervised data to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = tokenizer.texts_to_sequences(train_text) \n",
    "train_text = sequence.pad_sequences(train_text, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1178614, 40)\n"
     ]
    }
   ],
   "source": [
    "print(train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178614/1178614 [==============================] - 17s 15us/step\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.predict(train_text, batch_size=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.2 # 0.2:0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished appending!\n"
     ]
    }
   ],
   "source": [
    "semi_supervised_X = []\n",
    "semi_supervised_y = []\n",
    "for idx, pred in enumerate(preds):\n",
    "    if pred[0] > 1-margin:\n",
    "        semi_supervised_X.append(train_text[idx])\n",
    "        semi_supervised_y.append(1)\n",
    "    elif pred[0] < margin:\n",
    "        semi_supervised_X.append(train_text[idx])\n",
    "        semi_supervised_y.append(0)\n",
    "print('finished appending!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712673, 40)\n",
      "(200000, 40)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(semi_supervised_X).shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.append(X_train, semi_supervised_X, axis=0) # 200,000 --> 912,673\n",
    "y_train = np.append(y_train, semi_supervised_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912673,)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read test file, generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading file\n"
     ]
    }
   ],
   "source": [
    "x_submission = []\n",
    "\n",
    "with open('data/testing_data.txt', 'rt') as testfile:\n",
    "    reader = csv.reader(testfile, delimiter=',')\n",
    "    next(reader) # skip headings\n",
    "    for row in reader:\n",
    "        # print(''.join(row[1:]))\n",
    "        x_submission.append(''.join(row[1:]))\n",
    "print('finished reading file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 40\n",
    "x_submission = tokenizer.texts_to_sequences(x_submission)   \n",
    "x_submission = sequence.pad_sequences(x_submission, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0, 1181,    8,  314,   12,    1,   68,    5,  208,\n",
       "          1,  812, 1347,  141,  250,  800,   61], dtype=int32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_submission[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 40)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 6s 28us/step\n",
      "generated predictions with  <keras.models.Sequential object at 0x7f59198abe80>\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.predict(x_submission, batch_size=1024, verbose=1)\n",
    "print('generated predictions with ', loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.58626825]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeh = tokenizer.texts_to_sequences(['#'])\n",
    "yeh = sequence.pad_sequences(yeh, maxlen=max_review_length)\n",
    "loaded_model.predict(yeh, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing submission!\n"
     ]
    }
   ],
   "source": [
    "with open('punct_first_model.csv', 'wt') as outfile:\n",
    "    test_writer = csv.writer(outfile)\n",
    "    test_writer.writerow(['id','label'])\n",
    "    \n",
    "    counter = 0\n",
    "    for i in (preds > 0.5):\n",
    "        test_writer.writerow([counter, int(i)])\n",
    "        counter += 1\n",
    "    \n",
    "print('finished writing submission!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
