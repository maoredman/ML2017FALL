{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, concatenate, Input, Reshape, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import load_model\n",
    "import csv\n",
    "import os\n",
    "import errno\n",
    "import operator\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/train-v1.1.json', 'r') as f:\n",
    "     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('data/test-v1.1.json', 'r') as f:\n",
    "     data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1 data --> many paragraphs --> 1 context, many qas,\n",
    "context_qas_list = []\n",
    "for data_entry in data['data']:\n",
    "    for context_qas in data_entry['paragraphs']:\n",
    "        context_qas_list.append(context_qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '作為近代革命發源地之一，廣州自中華民國代時就是中國社會運動的中心之一。每次全國性的社會運動都有廣州民眾的響應和參與。以廣州為中心的較具規模的社會運動，最早有1925年至1926年在廣州和香港同時舉辦的省港大罷工。\\n廣州市民在1989年更發起活動聲援天安門民主運動，百萬人聚集海珠廣場圍繞廣州解放紀念碑集會。初期廣州媒體以「愛國運動」名義支持。流血事件發生後，民眾暴動，省市政府機關均被衝擊，所有廣州媒體隨即被禁言，亦干擾和封鎖香港電視台。省政府出動軍警鎮壓遊行群眾，大批參與者偷渡至香港、台灣及海外。各企事業單位亦派員審查各部門職工是否有組織或參與集會。\\n1999年的全國性反美活動，有數十萬群眾、學生在市內遊行示威，抗議北約轟炸中國駐南斯拉夫大使館，同時駐廣州美國領事館也受到部分激進示威人士破壞。廣州媒體對此進行全程跟進，但對廣州美國領館破壞情況則完全沒有提及。\\n2005年的全國性反日示威，也有數十萬人在主幹道遊行，不過廣州封鎖消息，大學和中學也禁止學生遊行，否則開除學籍。',\n",
       " 'qas': [{'answers': [{'answer_start': 93, 'text': '香港'}],\n",
       "   'id': 'e5d3c55d-41cd-42c6-93f9-e90186c38a7f',\n",
       "   'question': '省港大罷工是除了廣州以外還有哪個地區參與？'},\n",
       "  {'answers': [{'answer_start': 161, 'text': '愛國運動'}],\n",
       "   'id': '22a717ee-6026-4e1e-818b-1e1678b8947f',\n",
       "   'question': '廣州媒體聲援天安門民主運動一開始以甚麼名義？'},\n",
       "  {'answers': [{'answer_start': 311, 'text': '北約轟炸中國駐南斯拉夫大使館'}],\n",
       "   'id': '0897c133-c395-484f-b37d-d057ea8323a3',\n",
       "   'question': '在全國性反日示威的6年前的抗議活動是在抗議甚麼？'}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_qas_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# True (better F1 score): \"see if question word is in segment\"\n",
    "# False: \"see if segment word is in question\"\n",
    "find_question_chars_in_segment = True\n",
    "processing_train_data = True\n",
    "print_questions_and_segments = False\n",
    "append_another_segment_after = True # append another segment after most_similar_segment\n",
    "append_another_segment_before = False # append another segment before most_similar_segment\n",
    "\n",
    "if not processing_train_data: # test file\n",
    "    question_id = []\n",
    "    question_text = []\n",
    "    # submission_answer = []  ### for when I want to write a submission\n",
    "    segment_start_position = []\n",
    "    segment_text= []\n",
    "else: # train file\n",
    "    question_text = []\n",
    "    answer_start_position = []\n",
    "    answer_text = []\n",
    "    segment_start_position = []\n",
    "    segment_text= []\n",
    "\n",
    "for context_qas in context_qas_list:\n",
    "    context = context_qas['context']\n",
    "    context_segments = re.split('。', context) # prev: ，|。\n",
    "    # print(context)\n",
    "    \n",
    "    qas = context_qas['qas']\n",
    "    for qa in qas:\n",
    "        qa_id = qa['id']\n",
    "        qa_question = qa['question']\n",
    "        qa_answer_text = qa['answers'][0]['text']\n",
    "        qa_answer_start = qa['answers'][0]['answer_start']\n",
    "        \n",
    "        if processing_train_data:\n",
    "            question_text.append(qa_question)\n",
    "            answer_start_position.append(qa_answer_start)\n",
    "            answer_text.append(qa_answer_text)\n",
    "        \n",
    "        max_num_char_overlaps = 0\n",
    "        most_similar_segment = ''\n",
    "        most_similar_start_id = 0\n",
    "        current_start_id = 0\n",
    "        most_similar_segment_idx = 0\n",
    "        \n",
    "        for idx, context_segment in enumerate(context_segments):\n",
    "            if idx > 0:\n",
    "                current_start_id += len(context_segments[idx-1]) + 1 # add the ，or。removed by regex\n",
    "                \n",
    "            num_char_overlaps = 0\n",
    "            if find_question_chars_in_segment:\n",
    "                context_segment_chars = set([i for i in context_segment])\n",
    "                for question_char in qa_question:\n",
    "                    if question_char in context_segment_chars:\n",
    "                        num_char_overlaps += 1\n",
    "            else:\n",
    "                question_chars = set([i for i in qa_question])\n",
    "                for context_segment_char in context_segment:\n",
    "                    if context_segment_char in question_chars:\n",
    "                        num_char_overlaps += 1\n",
    "                    \n",
    "            if num_char_overlaps > max_num_char_overlaps:\n",
    "                max_num_char_overlaps = num_char_overlaps\n",
    "                most_similar_segment = context_segment\n",
    "                most_similar_start_id = current_start_id\n",
    "                most_similar_segment_idx = idx\n",
    "            \n",
    "            ### note: maybe try other similarity measures?\n",
    "            '''similarity_score = similarity(context_segment, qa['question'])\n",
    "            if similarity_score > max_similarity_score:\n",
    "                max_similarity_score = similarity_score\n",
    "                most_similar_segment = context_segment'''\n",
    "        \n",
    "        ### note: remove question chars from answer?\n",
    "        '''keep_char_in_segment = [True for i in range(0, len(most_similar_segment))] \n",
    "        for idx, question_char in enumerate(qa_question):'''\n",
    "            \n",
    "        \n",
    "        # print('question id: ', qa_id)\n",
    "        \n",
    "        if print_questions_and_segments:\n",
    "            print('question: ', qa_question)\n",
    "            print('answer: ', most_similar_segment)\n",
    "            print('should be same as answer: ', context[most_similar_start_id:most_similar_start_id+len(most_similar_segment)])\n",
    "        if not processing_train_data:\n",
    "            question_id.append(qa_id)\n",
    "            question_text.append(qa_question)\n",
    "            segment_text.append(most_similar_segment)\n",
    "            segment_start_position.append(most_similar_start_id)\n",
    "            # submission_answer.append(' '.join([str(i) for i in range(most_similar_start_id,most_similar_start_id+len(most_similar_segment))]))\n",
    "        else: # processing_train_data:\n",
    "            segment_start_position.append(most_similar_start_id)\n",
    "            if append_another_segment_after and most_similar_segment_idx+1 < len(context_segments):\n",
    "                segment_text.append(most_similar_segment + '。' + context_segments[most_similar_segment_idx+1])\n",
    "            elif append_another_segment_before and most_similar_segment_idx-1 >= 0:\n",
    "                segment_text.append(context_segments[most_similar_segment_idx-1] + '。' + most_similar_segment)\n",
    "            else:\n",
    "                segment_text.append(most_similar_segment)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check test file results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d08d2f0a-e8a2-4f5d-980c-5bad4a2b434b\n",
      "黃河為海河流域以及哪個流域的分水嶺?\n",
      "由於黃河泥沙量大，下遊河段長期淤積形成舉世聞名的「地上河」，黃河約束在大堤內成為海河流域與淮河流域的分水嶺\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "peek_idx = 5\n",
    "print(question_id[peek_idx])\n",
    "print(question_text[peek_idx])\n",
    "print(segment_text[peek_idx])\n",
    "print(segment_start_position[peek_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check train file results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_text: 位於廣州的白雲山有甚麼之稱？\n",
      "answer_start_position: 99\n",
      "answer_text: 市肺\n",
      "segment_start_position: 22\n",
      "segment_text: 廣州11個市轄區總面積7434.4平方公里。地勢東北高、西南低，背山面海，北部是森林集中的丘陵山區，最高峰為北部從化區與惠州市龍門縣交界處的天堂頂，海拔為1210米；東北部為中低山地，市區有被稱為「市肺」的白雲山；中部是丘陵盆地，南部為沿海沖積平原，為珠江三角洲的組成部分\n"
     ]
    }
   ],
   "source": [
    "peek_idx = 111\n",
    "print('question_text:', question_text[peek_idx])\n",
    "print('answer_start_position:', answer_start_position[peek_idx])\n",
    "print('answer_text:', answer_text[peek_idx])\n",
    "print('segment_start_position:', segment_start_position[peek_idx])\n",
    "print('segment_text:', segment_text[peek_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14611"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "bad_idx = []\n",
    "for idx, answer in enumerate(answer_text):\n",
    "    if answer_start_position[idx] > segment_start_position[idx] + len(segment_text[idx])-1\\\n",
    "    or answer_start_position[idx] + len(answer_text[idx])-1 < segment_start_position[idx]:\n",
    "        count += 1\n",
    "        bad_idx.append(idx)\n",
    "        '''print('question:', question_text[idx])\n",
    "        print('answer:', answer_text[idx])\n",
    "        print('segment:', segment_text[idx])'''\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13845732667168573\n"
     ]
    }
   ],
   "source": [
    "# 0.164 of 1-sentence segments do not cover correct answer\n",
    "# 0.127 of 2-sentence (extra sentence after) segments do not cover correct answer\n",
    "# 0.138 of 2-sentence (extra sentence before) segments do not cover correct answer\n",
    "print(count / len(answer_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes 10 seconds\n",
    "question_text = [question_text[i] for i in range(0, len(question_text)) if i not in bad_idx]\n",
    "answer_start_position = [answer_start_position[i] for i in range(0, len(answer_start_position)) if i not in bad_idx]\n",
    "answer_text = [answer_text[i] for i in range(0, len(answer_text)) if i not in bad_idx]\n",
    "segment_start_position = [segment_start_position[i] for i in range(0, len(segment_start_position)) if i not in bad_idx]\n",
    "segment_text= [segment_text[i] for i in range(0, len(segment_text)) if i not in bad_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12751"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all clear!\n"
     ]
    }
   ],
   "source": [
    "# check that bad training data have been removed\n",
    "for idx, answer in enumerate(answer_text):\n",
    "    if answer_start_position[idx] > segment_start_position[idx] + len(segment_text[idx])-1\\\n",
    "    or answer_start_position[idx] + len(answer_text[idx])-1 < segment_start_position[idx]:\n",
    "        bad_idx.append(idx)\n",
    "        print('question:', question_text[idx])\n",
    "        print('answer:', answer_text[idx])\n",
    "        print('segment:', segment_text[idx])\n",
    "print('all clear!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save test file segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'戴維提出了氮氣以及哪個單質在常溫常壓下為氣體?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(question_text)[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('question_id.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(question_id), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('question_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(question_text), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('segment_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(segment_text), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('segment_start_position.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(segment_start_position), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save train file segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('question_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(question_text), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('answer_start_position.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(answer_start_position), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('answer_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(answer_text), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('segment_start_position.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(segment_start_position), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('segment_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(np.array(segment_text), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## write submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing submission!\n"
     ]
    }
   ],
   "source": [
    "with open('char_overlap_sentence.csv', 'wt') as outfile:\n",
    "    test_writer = csv.writer(outfile)\n",
    "    test_writer.writerow(['id','answer'])\n",
    "    \n",
    "    for idx in range(0, len(submission_id)):\n",
    "        test_writer.writerow([submission_id[idx], submission_answer[idx]])\n",
    "    \n",
    "print('finished writing submission!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
